{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import re\n",
    "from jiwer import wer\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "TRAIN_DIR = \"/kaggle/input/turkish-speech-corpus/ISSAI_TSC_218/Train\"\n",
    "DEV_DIR = \"/kaggle/input/turkish-speech-corpus/ISSAI_TSC_218/Dev\"\n",
    "TEST_DIR = \"/kaggle/input/turkish-speech-corpus/ISSAI_TSC_218/Test\"\n",
    "\n",
    "chars_to_ignore = r'[,\\?\\.\\!\\-\\;:\"“%‘”�]'\n",
    "chars_to_mapping = {\"ğ\": \"g\", \"ı\": \"i\", \"ö\": \"o\", \"ü\": \"u\", \"ş\": \"s\", \"ç\": \"c\"}\n",
    "\n",
    "def normalize_text(text):\n",
    "    if text is None or not isinstance(text, str):\n",
    "        print(f\"Warning: normalize_text received invalid input: {text}\")\n",
    "        return \"\"\n",
    "    text = text.lower().strip()\n",
    "    for src, dst in chars_to_mapping.items():\n",
    "        text = text.replace(src, dst)\n",
    "    text = re.sub(chars_to_ignore, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "class AudioTextDataset(TorchDataset):\n",
    "    def __init__(self, folder, max_samples=100):\n",
    "        all_audio_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.wav')]\n",
    "        all_text_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.txt')]\n",
    "        all_audio_files.sort()\n",
    "        all_text_files.sort()\n",
    "        \n",
    "        self.audio_files = all_audio_files[:max_samples]\n",
    "        self.text_files = all_text_files[:max_samples]\n",
    "        \n",
    "        print(f\"Folder: {folder}\")\n",
    "        print(f\"Found {len(all_audio_files)} audio files, using {len(self.audio_files)}: {self.audio_files[:5]}\")\n",
    "        print(f\"Found {len(all_text_files)} text files, using {len(self.text_files)}: {self.text_files[:5]}\")\n",
    "        assert len(self.audio_files) > 0, f\"No .wav files in {folder}\"\n",
    "        assert len(self.text_files) > 0, f\"No .txt files in {folder}\"\n",
    "        assert len(self.audio_files) == len(self.text_files), \"Mismatch between audio and text files\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        text_path = self.text_files[idx]\n",
    "        try:\n",
    "            speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "            speech_array = librosa.resample(speech_array.squeeze().numpy(), orig_sr=sampling_rate, target_sr=16000)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio {audio_path}: {e}\")\n",
    "            return {\"speech\": np.array([]), \"sentence\": \"\", \"audio_file\": audio_path, \"text_file\": text_path}\n",
    "        \n",
    "        try:\n",
    "            with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "                transcription = normalize_text(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading text {text_path}: {e}\")\n",
    "            return {\"speech\": np.array([]), \"sentence\": \"\", \"audio_file\": audio_path, \"text_file\": text_path}\n",
    "        \n",
    "        return {\"speech\": speech_array, \"sentence\": transcription, \"audio_file\": audio_path, \"text_file\": text_path}\n",
    "\n",
    "# load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = AudioTextDataset(TRAIN_DIR, max_samples=1000)\n",
    "dev_dataset = AudioTextDataset(DEV_DIR, max_samples=200)\n",
    "test_dataset = AudioTextDataset(TEST_DIR, max_samples=100)\n",
    "\n",
    "# create vocabulary\n",
    "print(\"Creating vocabulary...\")\n",
    "vocab = set()\n",
    "for i in range(len(train_dataset)):\n",
    "    item = train_dataset[i]\n",
    "    if \"sentence\" in item and item[\"sentence\"]:\n",
    "        vocab.update(item[\"sentence\"])\n",
    "vocab_dict = {v: k for k, v in enumerate(sorted(vocab))}\n",
    "vocab_dict[\"|\"] = len(vocab_dict)\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "with open(\"vocab.json\", \"w\") as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n",
    "\n",
    "# init W&B run\n",
    "wandb.init(project=\"turkish-asr\", config={\n",
    "    \"model\": \"facebook/wav2vec2-large-xlsr-53\",\n",
    "    \"num_epochs\": 50,  \n",
    "    \"learning_rate\": 3e-4,  \n",
    "    \"batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"max_samples_train\": 1000,\n",
    "    \"max_samples_dev\": 200,\n",
    "    \"max_samples_test\": 100\n",
    "})\n",
    "\n",
    "# init model and processor\n",
    "print(\"Initializing Wav2Vec2 model and processor...\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    \"vocab.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"|\"\n",
    ")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# update model's config and reinitialize lm_head\n",
    "model.config.vocab_size = len(vocab_dict)\n",
    "model.lm_head = torch.nn.Linear(model.config.hidden_size, len(vocab_dict))\n",
    "torch.nn.init.xavier_uniform_(model.lm_head.weight)\n",
    "\n",
    "# freeze lower layers to focus training on lm_head and upper layers\n",
    "for param in model.wav2vec2.feature_extractor.parameters():\n",
    "    param.requires_grad = False  # freeze feature extractor\n",
    "for i, layer in enumerate(model.wav2vec2.encoder.layers):\n",
    "    if i < 20:  # freeze first 20 of 24 layers\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# move model to gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def data_collator(batch):\n",
    "    valid_items = [item for item in batch if item[\"speech\"].size > 0 and isinstance(item[\"sentence\"], str)]\n",
    "    if not valid_items:\n",
    "        print(\"Warning: No valid items in batch, skipping...\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        input_values = processor(\n",
    "            [item[\"speech\"] for item in valid_items],\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).input_values\n",
    "        \n",
    "        labels = processor.tokenizer(\n",
    "            [item[\"sentence\"] for item in valid_items],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).input_ids\n",
    "        \n",
    "        print(f\"Labels shape: {labels.shape}, dtype: {labels.dtype}\")\n",
    "        if not torch.is_tensor(labels) or labels.dtype != torch.int64:\n",
    "            raise ValueError(\"Labels are not a proper tensor of integers\")\n",
    "        \n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"labels\": labels,\n",
    "            \"audio_files\": [item[\"audio_file\"] for item in valid_items],\n",
    "            \"text_files\": [item[\"text_file\"] for item in valid_items]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in data_collator: {e}\")\n",
    "        print(f\"Sentences passed to tokenizer: {[item['sentence'] for item in valid_items]}\")\n",
    "        return None\n",
    "\n",
    "# create DataLoaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "eval_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# training parameters\n",
    "num_epochs = 50  \n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "# evaluation function\n",
    "def evaluate(loader, model, processor, device, dataset_name=\"eval\"):\n",
    "    model.eval()\n",
    "    total_wer = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if batch is None:\n",
    "                print(f\"Skipping {dataset_name} batch due to invalid items\")\n",
    "                continue\n",
    "            input_values = batch[\"input_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_values)\n",
    "            pred_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "            pred_str = processor.batch_decode(pred_ids)\n",
    "            label_str = processor.batch_decode(labels, group_tokens=False)\n",
    "            \n",
    "            print(f\"{dataset_name} Batch - Raw pred_ids: {pred_ids.tolist()[:2]}\")\n",
    "            print(f\"{dataset_name} Batch - Logits max/min: {outputs.logits.max().item():.2f}/{outputs.logits.min().item():.2f}\")\n",
    "            print(f\"{dataset_name} Batch - Predicted: {pred_str}\")\n",
    "            print(f\"{dataset_name} Batch - Ground Truth: {label_str}\")\n",
    "            \n",
    "            batch_wer = wer(label_str, pred_str)\n",
    "            total_wer += batch_wer\n",
    "            num_batches += 1\n",
    "    avg_wer = total_wer / num_batches if num_batches > 0 else float('inf')\n",
    "    model.train()\n",
    "    return avg_wer\n",
    "\n",
    "# training loop\n",
    "print(\"Starting fine-tuning...\")\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        if batch is None:\n",
    "            print(f\"Skipping batch {i} due to invalid items\")\n",
    "            continue\n",
    "        \n",
    "        input_values = batch[\"input_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(input_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            steps += 1\n",
    "            wandb.log({\"train/loss\": loss.item(), \"train/step\": steps + epoch * len(train_loader)})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # eval on dev set after each epoch\n",
    "    dev_wer = evaluate(eval_loader, model, processor, device, \"dev\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Dev WER: {dev_wer:.4f}\")\n",
    "    \n",
    "    # eval on test set after each epoch\n",
    "    test_wer = evaluate(test_loader, model, processor, device, \"test\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Test WER: {test_wer:.4f}\")\n",
    "    \n",
    "    # log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"train/avg_loss\": avg_loss,\n",
    "        \"dev/avg_wer\": dev_wer,\n",
    "        \"test/avg_wer\": test_wer,\n",
    "        \"epoch\": epoch + 1\n",
    "    })\n",
    "\n",
    "# final evaluation on dev set\n",
    "print(\"Final evaluation on dev set...\")\n",
    "dev_wer = evaluate(eval_loader, model, processor, device, \"dev\")\n",
    "print(f\"Final Average WER on dev set: {dev_wer:.4f}\")\n",
    "wandb.log({\"dev/final_avg_wer\": dev_wer})\n",
    "\n",
    "# final evaluation on test set with results \n",
    "# saving and W&B logging\n",
    "print(\"Final evaluation on test set...\")\n",
    "model.eval()\n",
    "total_wer = 0\n",
    "num_batches = 0\n",
    "test_results = []\n",
    "test_table = wandb.Table(columns=[\"audio_file\", \"text_file\", \"ground_truth\", \"prediction\", \"wer\", \"pred_ids\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        if batch is None:\n",
    "            print(\"Skipping test batch due to invalid items\")\n",
    "            continue\n",
    "        \n",
    "        input_values = batch[\"input_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        audio_files = batch[\"audio_files\"]\n",
    "        text_files = batch[\"text_files\"]\n",
    "        \n",
    "        outputs = model(input_values)\n",
    "        pred_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "        pred_str = processor.batch_decode(pred_ids)\n",
    "        label_str = processor.batch_decode(labels, group_tokens=False)\n",
    "        \n",
    "        print(f\"Test Batch - Raw pred_ids: {pred_ids.tolist()[:2]}\")\n",
    "        print(f\"Test Batch - Logits max/min: {outputs.logits.max().item():.2f}/{outputs.logits.min().item():.2f}\")\n",
    "        print(f\"Test Batch - Predicted: {pred_str}\")\n",
    "        print(f\"Test Batch - Ground Truth: {label_str}\")\n",
    "        \n",
    "        batch_wer = wer(label_str, pred_str)\n",
    "        total_wer += batch_wer\n",
    "        num_batches += 1\n",
    "        \n",
    "        for i in range(len(pred_str)):\n",
    "            sample_wer = batch_wer if len(pred_str) == 1 else wer([label_str[i]], [pred_str[i]])\n",
    "            test_results.append({\n",
    "                \"audio_file\": audio_files[i],\n",
    "                \"text_file\": text_files[i],\n",
    "                \"ground_truth\": label_str[i],\n",
    "                \"prediction\": pred_str[i],\n",
    "                \"wer\": sample_wer\n",
    "            })\n",
    "            test_table.add_data(\n",
    "                audio_files[i],\n",
    "                text_files[i],\n",
    "                label_str[i],\n",
    "                pred_str[i],\n",
    "                sample_wer,\n",
    "                str(pred_ids[i].tolist())\n",
    "            )\n",
    "        \n",
    "        wandb.log({\"test/batch_wer\": batch_wer})\n",
    "\n",
    "avg_wer = total_wer / num_batches if num_batches > 0 else float('inf')\n",
    "print(f\"Final Average WER on test set: {avg_wer:.4f}\")\n",
    "wandb.log({\"test/final_avg_wer\": avg_wer})\n",
    "\n",
    "# log test results table to W&B\n",
    "wandb.log({\"test/predictions\": test_table})\n",
    "\n",
    "# save test results to JSON\n",
    "results_dict = {\n",
    "    \"test_results\": test_results,\n",
    "    \"average_wer\": avg_wer,\n",
    "    \"num_samples\": len(test_results),\n",
    "    \"num_batches\": num_batches\n",
    "}\n",
    "with open(\"test_results.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(results_dict, f, ensure_ascii=False, indent=4)\n",
    "print(\"Test results saved to 'test_results.json'\")\n",
    "\n",
    "# save the fine-tuned model and processor\n",
    "print(\"Saving fine-tuned model and processor...\")\n",
    "model.save_pretrained(\"./model/wav2vec\")\n",
    "processor.save_pretrained(\"./model/wav2vec\")\n",
    "\n",
    "# finish W&B run\n",
    "wandb.finish()\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6981017,
     "sourceId": 11183746,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
