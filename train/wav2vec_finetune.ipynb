{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 1: Install Dependencies\n",
    "# Run this in your terminal or as a code cell:\n",
    "# ```bash\n",
    "# pip install transformers datasets torchaudio librosa jiwer\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 2: Import Libraries\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\n",
    "import librosa\n",
    "import numpy as np\n",
    "import re\n",
    "from jiwer import wer\n",
    "import json\n",
    "from utils.log import logger  # Assuming you have a logger setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 3: Define Text Normalization for Turkish\n",
    "# Turkish has specific characters and needs normalization for ASR.\n",
    "chars_to_ignore = r'[,\\?\\.\\!\\-\\;:\"“%‘”�]'\n",
    "chars_to_mapping = {\"ğ\": \"g\", \"ı\": \"i\", \"ö\": \"o\", \"ü\": \"u\", \"ş\": \"s\", \"ç\": \"c\"}\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize Turkish text for ASR.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    for src, dst in chars_to_mapping.items():\n",
    "        text = text.replace(src, dst)\n",
    "    text = re.sub(chars_to_ignore, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 4: Prepare Dataset\n",
    "\n",
    "# Load the Common Voice Turkish dataset and preprocess it.\n",
    "logger.info(\"Loading Turkish dataset...\")\n",
    "dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"tr\", split=\"train+validation\")\n",
    "\n",
    "# Preprocess dataset\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"Preprocess audio and text for training.\"\"\"\n",
    "    # Load audio\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    speech_array = librosa.resample(speech_array.squeeze().numpy(), orig_sr=sampling_rate, target_sr=16000)\n",
    "    batch[\"speech\"] = speech_array\n",
    "    # Normalize transcription\n",
    "    batch[\"sentence\"] = normalize_text(batch[\"sentence\"])\n",
    "    return batch\n",
    "\n",
    "logger.info(\"Preprocessing dataset...\")\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=[\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "\n",
    "# Split into train and validation (90% train, 10% validation)\n",
    "train_dataset = dataset.shuffle(seed=42).select(range(int(len(dataset) * 0.9)))\n",
    "eval_dataset = dataset.shuffle(seed=42).select(range(int(len(dataset) * 0.9), len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 5: Create Vocabulary\n",
    "# Create a vocabulary based on the training dataset.\n",
    "vocab = set(\"\".join(train_dataset[\"sentence\"]))\n",
    "vocab_dict = {v: k for k, v in enumerate(sorted(vocab))}\n",
    "vocab_dict[\"|\"] = len(vocab_dict)  # Space token\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)  # Unknown token\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)  # Padding token\n",
    "\n",
    "# Save vocab to a file\n",
    "with open(\"vocab.json\", \"w\") as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## Step 6: Initialize Model and Processor\n",
    "logger.info(\"Initializing Wav2Vec2 model and processor...\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\", vocab_file=\"vocab.json\")\n",
    "\n",
    "# Update the processor's tokenizer with the new vocab\n",
    "processor.tokenizer = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\", vocab_file=\"vocab.json\").tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 7: Define Data Collator\n",
    "def data_collator(batch):\n",
    "    \"\"\"Collate data for training.\"\"\"\n",
    "    input_values = processor([item[\"speech\"] for item in batch], sampling_rate=16000, return_tensors=\"pt\", padding=True).input_values\n",
    "    labels = processor.tokenizer([item[\"sentence\"] for item in batch], return_tensors=\"pt\", padding=True).input_ids\n",
    "    return {\"input_values\": input_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 8: Define Evaluation Metric (WER)\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute Word Error Rate (WER) for evaluation.\"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    wer_score = wer(label_str, pred_str)\n",
    "    return {\"wer\": wer_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 9: Set Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model/wav2vec\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use mixed precision if GPU supports it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 10: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 11: Train the Model\n",
    "logger.info(\"Starting fine-tuning...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 12: Save the Fine-Tuned Model and Processor\n",
    "logger.info(\"Saving fine-tuned model and processor...\")\n",
    "model.save_pretrained(\"./model/wav2vec\")\n",
    "processor.save_pretrained(\"./model/wav2vec\")\n",
    "\n",
    "logger.info(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
