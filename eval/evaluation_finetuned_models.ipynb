{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "!pip install torch torchaudio transformers librosa jiwer nltk rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "from jiwer import wer\n",
    "import os\n",
    "import difflib\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# path to finetuned model dir\n",
    "MODEL_PATH = \"/kaggle/input/med-asr-whisper-finetune/model/whisper\" #\"/kaggle/input/med-wav2vec-asr-finetune/model/wav2vec\"\n",
    "\n",
    "# load the fine-tuned model and processor\n",
    "try:\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(MODEL_PATH)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or processor from {MODEL_PATH}: {e}\")\n",
    "    raise\n",
    "\n",
    "# use gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# set eval model for inference\n",
    "model.eval()  \n",
    "\n",
    "def transcribe_audio(audio_path: str) -> str:\n",
    "    \"\"\"Transcribes audio using the fine-tuned Wav2Vec2 model.\"\"\"\n",
    "    try:\n",
    "        # load audio file\n",
    "        speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "        # remove channel dimension\n",
    "        speech_array = speech_array.squeeze().numpy()  \n",
    "        \n",
    "        # resample to 16kHz as model expects\n",
    "        if sampling_rate != 16000:\n",
    "            speech_array = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=16000)\n",
    "        \n",
    "        # preprocess audio with the processor\n",
    "        inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        input_values = inputs.input_values.to(device)\n",
    "        \n",
    "        # inference\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "        \n",
    "        # decode predicted ids to text\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(pred_ids)[0]\n",
    "        \n",
    "        return transcription\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {audio_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def transcribe_and_refine(audio_path: str) -> str:\n",
    "    \"\"\"Transcribes and applies basic refinement.\"\"\"\n",
    "    transcription = transcribe_audio(audio_path)\n",
    "    if transcription:\n",
    "        # remove extra spaces\n",
    "        transcription = \" \".join(transcription.split())\n",
    "    return transcription\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Cleans the given text for better comparison.\"\"\"\n",
    "    return (\n",
    "        text.lower()\n",
    "            .replace(\"-\", \" \")\n",
    "            .replace(\"\\u2022\", \"\")\n",
    "            .replace(\"*\", \"\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\",\", \" \")\n",
    "            .replace(\".\", \" \")\n",
    "            .replace(\";\", \" \")\n",
    "            .replace(\":\", \" \")\n",
    "            .strip()\n",
    "    )\n",
    "\n",
    "def evaluate_test_set(test_loader, model, processor, device):\n",
    "    \"\"\"Evaluates the test set using the fine-tuned model and computes multiple metrics.\"\"\"\n",
    "    print(\"Final evaluation on test set...\")\n",
    "    model.eval()\n",
    "    total_wer = 0\n",
    "    num_batches = 0\n",
    "    test_results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if batch is None:\n",
    "                print(\"Skipping test batch due to invalid items\")\n",
    "                continue\n",
    "            \n",
    "            input_values = batch[\"input_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            audio_files = batch[\"audio_files\"]\n",
    "            text_files = batch[\"text_files\"]\n",
    "            \n",
    "            # inference\n",
    "            outputs = model(input_values)\n",
    "            pred_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "            pred_str = processor.batch_decode(pred_ids)\n",
    "            label_str = processor.batch_decode(labels, group_tokens=False)\n",
    "            \n",
    "            # calculate WER for the batch\n",
    "            batch_wer = wer(label_str, pred_str)\n",
    "            total_wer += batch_wer\n",
    "            num_batches += 1\n",
    "            \n",
    "            # other metrics for each sample\n",
    "            for i in range(len(pred_str)):\n",
    "                gt_clean = clean_text(label_str[i])\n",
    "                pred_clean = clean_text(pred_str[i])\n",
    "                \n",
    "                # WER\n",
    "                sample_wer = batch_wer if len(pred_str) == 1 else wer([label_str[i]], [pred_str[i]])\n",
    "                \n",
    "                # SequenceMatcher\n",
    "                sequence_score = difflib.SequenceMatcher(None, gt_clean, pred_clean).ratio()\n",
    "                \n",
    "                # BLEU\n",
    "                smoothie = SmoothingFunction().method4\n",
    "                bleu = sentence_bleu([gt_clean.split()], pred_clean.split(), smoothing_function=smoothie)\n",
    "                \n",
    "                # ROUGE-L\n",
    "                rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "                rouge_score_l = rouge.score(gt_clean, pred_clean)['rougeL'].fmeasure\n",
    "                \n",
    "                # BERTScore\n",
    "                P, R, F1 = bert_score([pred_clean], [gt_clean], lang=\"en\", rescale_with_baseline=True)\n",
    "                bert_f1 = F1[0].item()\n",
    "                \n",
    "                # results\n",
    "                test_results.append({\n",
    "                    \"audio_file\": audio_files[i],\n",
    "                    \"text_file\": text_files[i],\n",
    "                    \"ground_truth\": label_str[i],\n",
    "                    \"prediction\": pred_str[i],\n",
    "                    \"wer\": round(sample_wer, 4),\n",
    "                    \"similarity_score\": round(sequence_score, 4),\n",
    "                    \"bleu\": round(bleu, 4),\n",
    "                    \"rougeL\": round(rouge_score_l, 4),\n",
    "                    \"bert_score_f1\": round(bert_f1, 4)\n",
    "                })\n",
    "            \n",
    "            print(f\"Test Batch - Predicted: {pred_str}\")\n",
    "            print(f\"Test Batch - Ground Truth: {label_str}\")\n",
    "            print(f\"Test Batch - WER: {batch_wer:.4f}\")\n",
    "\n",
    "    # average WER\n",
    "    avg_wer = total_wer / num_batches if num_batches > 0 else float('inf')\n",
    "    print(f\"Final Average WER on test set: {avg_wer:.4f}\")\n",
    "\n",
    "    # save results\n",
    "    results_dict = {\n",
    "        \"test_results\": test_results,\n",
    "        \"average_wer\": round(avg_wer, 4),\n",
    "        \"num_samples\": len(test_results),\n",
    "        \"num_batches\": num_batches\n",
    "    }\n",
    "    with open(\"test_results.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(results_dict, f, ensure_ascii=False, indent=4)\n",
    "    print(\"Test results saved to 'test_results.json'\")\n",
    "\n",
    "# dataset and loader\n",
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, audio_files, text_files):\n",
    "        self.audio_files = audio_files\n",
    "        self.text_files = text_files\n",
    "        assert len(self.audio_files) == len(self.text_files), \"Mismatch between audio and text files\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        text_path = self.text_files[idx]\n",
    "        try:\n",
    "            speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "            speech_array = librosa.resample(speech_array.squeeze().numpy(), orig_sr=sampling_rate, target_sr=16000)\n",
    "            with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "            return {\n",
    "                \"speech\": speech_array,\n",
    "                \"text\": text,\n",
    "                \"audio_files\": audio_path,\n",
    "                \"text_files\": text_path\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_path} or {text_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # filter None items\n",
    "    batch = [item for item in batch if item is not None] \n",
    "    if not batch:\n",
    "        return None\n",
    "    \n",
    "    # process audio batch with padding\n",
    "    speech_arrays = [item[\"speech\"] for item in batch]\n",
    "    inputs = processor(speech_arrays, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs.input_values\n",
    "    \n",
    "    # process text batch with padding\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    labels = processor.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids\n",
    "    \n",
    "    return {\n",
    "        \"input_values\": input_values,\n",
    "        \"labels\": labels,\n",
    "        \"audio_files\": [item[\"audio_files\"] for item in batch],\n",
    "        \"text_files\": [item[\"text_files\"] for item in batch]\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # test data paths\n",
    "    audio_dir = \"/kaggle/input/med-test-ebnchmark/test/test_voice_data\"\n",
    "    text_dir = \"/kaggle/input/med-test-ebnchmark/test/test_text_data\"\n",
    "    test_audio_files = [os.path.join(audio_dir, f) for f in sorted(os.listdir(audio_dir)) if f.endswith('.mp3') or f.endswith('.wav')]\n",
    "    test_text_files = [os.path.join(text_dir, f) for f in sorted(os.listdir(text_dir)) if f.endswith('.txt')]\n",
    "\n",
    "    # dataset and loader\n",
    "    test_dataset = AudioTextDataset(test_audio_files, test_text_files)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # evaluation\n",
    "    evaluate_test_set(test_loader, model, processor, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
